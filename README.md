# Multi-armed-bandits
Multi-armed bandits experiment with epsilon-greedy, UCB, Thompson sampling, Bayesian-greedy and HA-UCB
Run bandits.py with 2 parameters from console: number of arms and number of episodes.
  - \> python bandits.py 5 1000
Jupyter file shows experiment results.
